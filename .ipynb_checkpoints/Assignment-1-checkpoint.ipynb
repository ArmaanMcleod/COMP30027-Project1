{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bilal Shehata and Armaan Mcleoud \n",
    "--DREAM TEAM--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS CELL TO LOAD IN FUNCTIONS\n",
    "\n",
    "#imports used for program\n",
    "\n",
    "import itertools\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "from random import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# This function should open a data file in csv, and transform it into a usable format \n",
    "\n",
    "def k_fold(fulldf):\n",
    "    #split array into 10 pieces\n",
    "    karrays = np.array_split(fulldf,K)\n",
    "  \n",
    "    counter = 0 \n",
    "    sum = 0 #record results\n",
    "    \n",
    "    for i in range(0,len(karrays)):\n",
    "        counter = 0\n",
    "        testdf = karrays[i] #set the test array as one of the chunks\n",
    "        \n",
    "       \n",
    "        for j in range(0,len(karrays)):\n",
    "            #ensure that were not adding the test chunk to the array \n",
    "            if i != j: \n",
    "                \n",
    "               if counter == 0:\n",
    "                   counter+=1\n",
    "                   traindf = karrays[j] #initialise the data to be trained\n",
    "                   continue \n",
    "               #concatinate all the chunks that arent the test chunk\n",
    "               traindf = pd.concat([traindf,karrays[j]],axis = 0) \n",
    "        \n",
    "        #train the classifer by building the probability dictionary \n",
    "        probs, classes, priors = train_supervised(traindf)\n",
    "        #evaluate the classifier \n",
    "    \n",
    "        sum+= evaluate_supervised(testdf, probs, classes,priors)\n",
    "        \n",
    "    return sum/K       \n",
    "\n",
    "#determinisatically decide classes for instances \n",
    "def deterministic_preprocess():\n",
    "    #load in the dataframe \n",
    "     df = pd.read_csv(filepath, header = None)\n",
    "    \n",
    "     clean(df) #impute missing values if they exist\n",
    "     df = df.sample(frac=1).reset_index(drop=True) #shuffles the dataframe\n",
    "     classes = df[len(df.columns)-1].unique()\n",
    "    #remove classes from the data \n",
    "     df = df.iloc[:, :-1]\n",
    "    \n",
    "    \n",
    "    #randomly select a class for the instance \n",
    "     df[len(df.columns)-1] = np.random.choice(classes, df.shape[0])\n",
    "     \n",
    "    \n",
    "   \n",
    "   \n",
    "     return df, classes\n",
    "    \n",
    "#preprocessing for supervised classifier \n",
    "def preprocess():\n",
    "    \n",
    "    \n",
    "    #load in data \n",
    "    df = pd.read_csv(filepath, header = None)\n",
    "\n",
    "    clean(df) #impute missing values if they exist\n",
    "    df = df.sample(frac=1).reset_index(drop=True) #shuffles the dataframe\n",
    "  \n",
    "    \n",
    "    return df\n",
    "\n",
    "               \n",
    "           \n",
    "def train_supervised(df):\n",
    "    \n",
    "    #prior probabilties\n",
    "    highProb = df.groupby(len(df.columns)-1).size().div(len(df))\n",
    "     \n",
    "    #probabiltiy deictionary \n",
    "    probList = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    #this loop calculates the probabilties \n",
    "    for i in range(0,len(df.columns)-1):\n",
    "        cols =[len(df.columns)-1,i]\n",
    "       \n",
    "        trained = df.groupby(cols).size().div(len(df)).multiply(highProb, axis = 0,level=(len(df.columns)-1))\n",
    "        \n",
    "        probList[i] = trained\n",
    "    \n",
    "    #converts dataframe into dictionary \n",
    "    for i in range(0,len(probList)):\n",
    "        probList[i] = probList[i].to_dict()\n",
    "        #collects the classes into an list\n",
    "    classes = df[len(df.columns)-1].unique()\n",
    "    \n",
    "    priors = get_super_priors(df,classes)\n",
    "    \n",
    "    return probList, classes, priors\n",
    "\n",
    "def evaluate_supervised(testcsv, probs,classes,priors):\n",
    "\n",
    "    correct = 0\n",
    "    total = 0 \n",
    "    #iterate over each of the rows and pass it to the predict supervised method\n",
    "    for index, testrow in testcsv.iterrows() :\n",
    "        \n",
    "        #if the prediction is correct increase the counter \n",
    "        if predict_supervised(probs, testrow.tolist(),classes,priors) == testrow[len(testcsv.columns)-1]:\n",
    "            \n",
    "            correct +=1\n",
    "     \n",
    "        total +=1\n",
    "        \n",
    "     #return the accuracy of the classifier \n",
    "    return correct/total\n",
    "            \n",
    "#cleaning methiod removes '?' and places in it the most common value for that column\n",
    "def clean(dataframe):\n",
    "    #find all ? values and impute the most common value for that attribute \n",
    "    for index, testrow in dataframe.iterrows():\n",
    "      for i in range(0,len(testrow)):\n",
    "          if testrow[i] == '?':\n",
    "              testrow[i] = dataframe[i].value_counts().idxmax()\n",
    "              dataframe.loc[index,dataframe.columns[i] ] = testrow[i]\n",
    "  \n",
    "    return dataframe\n",
    "     \n",
    "\n",
    "    \n",
    "    #train the data\n",
    "  \n",
    "\n",
    "#function is used for preprocessing the data for unsupervised method \n",
    "def unsupervised_preprocess():\n",
    "      df = pd.read_csv(filepath, header = None)\n",
    "      \n",
    "      df = clean(df) #impute missing values if they exist\n",
    "      df = df.sample(frac=1).reset_index(drop=True) #shuffles the dataframe\n",
    "      classes = sorted(df[len(df.columns)-1].unique()) #returns the set of classes from dataframe\n",
    "   \n",
    "      df = df.iloc[:, :-1] #for unsupervised method the classes are cropped off the dataframe \n",
    "      #for each row and class assign a random value to the classes\n",
    "      for probable_class in classes:\n",
    "          df[probable_class] = pd.Series(np.random.rand(len(df)))\n",
    "      #ensure that the values are normalised\n",
    "      normalise_unsupervised(df,classes)\n",
    "      \n",
    "   \n",
    "      return df, classes\n",
    "#function is used to normalise the distributions of the pandas file in the dataframe     \n",
    "def normalise_unsupervised(df, classes):\n",
    "    #iterate through the rows\n",
    "    for index, row in df.iterrows():\n",
    "        #iterate through the attributes which are classes\n",
    "        total = sum(row[-len(classes):])\n",
    "        for i in range((len(row)-len(classes)),len(row)):\n",
    "            temp= float(row[i])/total\n",
    "            df.loc[index,df.columns[i] ] = temp\n",
    "      \n",
    "   \n",
    "    return\n",
    "\n",
    "#function to return the prior probablities in the supervised method\n",
    "def get_super_priors(df,classes):\n",
    "    priors = [0.0] * len(classes)\n",
    "    total = 0\n",
    "    classes = classes.tolist()\n",
    "    for index, row in df.iterrows():\n",
    "        priors[classes.index(row[len(row)-1])] += 1 \n",
    "        total += 1\n",
    "    for i in range(len(priors)):\n",
    "        priors[i] = priors[i]/total\n",
    "\n",
    "           \n",
    "      \n",
    "   \n",
    "    return priors\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# This function should build an unsupervised NB model \n",
    "def train_unsupervised(df, classes):\n",
    "   \n",
    "    #collect priors \n",
    "    priors= get_priors(df,classes)\n",
    "    \n",
    "    #this value indictates the number of iterations  \n",
    "    iterations = 4\n",
    "    for i in range(1,iterations): \n",
    "       #create the initial dictionary \n",
    "        probList = make_probability_dictionary(df,classes)\n",
    "        #use divide the values by fractional values \n",
    "        probList = factional_divide(df,classes,probList)\n",
    "        #reassign the distrubitions \n",
    "        df = assign_distro(df,classes,probList,priors)\n",
    "\n",
    "    \n",
    "    return probList, classes, priors\n",
    "\n",
    "\n",
    "#this function edits the probablity dictionary by fractionally diving the value s\n",
    "def factional_divide(df,classes,probList):\n",
    "    #get the fractional totals \n",
    "    fraccounts = get_fraccounts(df,classes)\n",
    "    #for every attribute\n",
    "    for i in probList.keys():\n",
    "        #for every class\n",
    "        for classtype in probList[i].keys():\n",
    "            #for every attribute value \n",
    "            for value in probList[i][classtype].keys():\n",
    "                    #divide the probability by the fractional count for that probabily \n",
    "                    probList[i][classtype][value] = probList[i][classtype][value] / fraccounts[classes.index(classtype)]\n",
    "\n",
    "    return probList\n",
    "\n",
    "#creates a list of fractional counts \n",
    "def get_fraccounts(df,classes):\n",
    "    fraccounts = []\n",
    "    for i in classes:   \n",
    "        #add the sum of each class column to list \n",
    "        fraccounts.append(sum(df[i]))\n",
    "    \n",
    "    return fraccounts\n",
    "#this function builds a probability dictionary \n",
    "def make_probability_dictionary(df,classes):\n",
    "    \n",
    "     probList = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda:0.00000001)))\n",
    "     \n",
    "     for prob_class in range(0,len(classes)):\n",
    "        for index, row in df.iterrows() :\n",
    "            for i in range(0,len(row)-len(classes)):\n",
    "                probList[i][classes[prob_class]][row[i]] +=row[len(row)-len(classes)+prob_class]\n",
    "     return probList\n",
    "\n",
    "#assigns the distrobution for unsupervised instances \n",
    "def assign_distro(df,classes,probList,classProbs):\n",
    "     for index, row in df.iterrows():\n",
    "       \n",
    "       #iterate through the instances assigning new values for each class \n",
    "       for probable_Class in range(len(classes)):\n",
    "           \n",
    "           product = 1 * classProbs[probable_Class]\n",
    "           \n",
    "           for attrib in range((len(row)-len(classes))):    \n",
    "              \n",
    "               product = product *probList[attrib][classes[probable_Class]][row[attrib]]\n",
    "               \n",
    "           df.loc[index,df.columns[(len(row)-len(classes) + probable_Class)]] = product\n",
    "    #normalise the values so they add to 1 \n",
    "     normalise_unsupervised(df,classes)\n",
    "     \n",
    "     return df\n",
    "\n",
    "#returns the values for the prior probablities \n",
    "def get_priors(df,classes):\n",
    "    classProbs = []\n",
    "    #create a list for the probablities for each of the classes \n",
    "    for i in classes:   \n",
    "        \n",
    "        classProbs.append(sum(df[i])/len(df))\n",
    "    \n",
    "    return classProbs\n",
    "#class makes predicitons for supervised method \n",
    "def predict_supervised(probList,testrow,classes,priors):\n",
    "    \n",
    "    classChance = [0.0] * len(classes)#keep record of all classchanes for this row\n",
    "    classlist = classes.tolist()\n",
    "    for possibleClass in classes : \n",
    "        #multiple the prior probablity \n",
    "        prob =1 *priors[classlist.index(possibleClass)]\n",
    "        for i in range(0,len(testrow)-1) : #for every element multiply in its conditional probablity \n",
    "  \n",
    "            if (possibleClass,testrow[i]) in probList[i]: \n",
    "                #multiply in condition probability \n",
    "                prob = prob * probList[i][(possibleClass,testrow[i])]\n",
    "            else : \n",
    "               \n",
    "                prob = prob * 0.00000000000001 #epsilon \n",
    "        \n",
    "        classChance[classlist.index(possibleClass)]= prob\n",
    "        \n",
    "        \n",
    "\n",
    "            #here we get the highest probability and match it to the class\n",
    "    \n",
    "    #the class with the highest probablity is what is predicted  \n",
    "    return classes[classChance.index(max(classChance))]\n",
    "\n",
    "# This function should predict the class distribution for a set of instances, based on a trained model\n",
    "def predict_unsupervised(probList,testrow,classes,priors):\n",
    "    \n",
    "    shuffle(classes)  \n",
    "    classChance = [0.0] * len(classes)#keep record of all classchanes for this row\n",
    "    classlist = classes\n",
    "    for possibleClass in classes : \n",
    "        #multiply in prior probability  \n",
    "        prob =1 *priors[classlist.index(possibleClass)]\n",
    "        for i in range(0,len(testrow)-len(classes)) : #for every element multiply in  its conditional probability \n",
    "  \n",
    "            if testrow[i] in probList[i][possibleClass]: \n",
    "                  #multiply in the condition probablity \n",
    "                prob = prob * probList[i][possibleClass][testrow[i]]\n",
    "            else: \n",
    "               \n",
    "               prob = prob * 0.0000000001 #epsilon \n",
    "        classChance[classlist.index(possibleClass)]= prob\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    return classes[classChance.index(max(classChance))]\n",
    "  \n",
    "#uses confusion matrix to evaluate unsupervised classifier\n",
    "def evaluate_unsupervised(testcsv, probs, classes,priors):\n",
    "    \n",
    "    \n",
    "    correct = 0\n",
    "    total = 0 \n",
    "    guesses = list()\n",
    "    trues = list()\n",
    "    #iterate over each of the rows and pass it to the predict supervised method\n",
    "    for index, testrow in testcsv.iterrows() :\n",
    "        \n",
    "            \n",
    "        #correct predictions increase the correct count\n",
    "        if predict_unsupervised(probs, testrow.tolist(),classes,priors) == testrow[len(testcsv.columns)-1]:\n",
    "            \n",
    "            correct +=1\n",
    "        #add the guesses to a list\n",
    "        guesses.append(predict_unsupervised(probs, testrow.tolist(),classes,priors))\n",
    "        #add the true values to a list \n",
    "        trues.append(testrow[len(testcsv.columns)-1])\n",
    "        \n",
    "        total +=1\n",
    "    #this section generates a confusion matrix ----------------\n",
    "    cm = confusion_matrix(trues, guesses, labels=classes)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] #normalise the values \n",
    "    plt.imshow(cm,cmap= plt.cm.Greens) #use the green colour scheme \n",
    "  \n",
    "    plt.colorbar() #add the colour bar \n",
    "    tick_marks = np.arange(len(classes)) \n",
    "    plt.xticks(tick_marks,classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    width, height = cm.shape\n",
    "\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            plt.annotate('%.4f' % cm[x][y], xy=(y, x), #show 4 digits after decimal \n",
    "                        horizontalalignment='center',\n",
    "                        verticalalignment='center')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "        \n",
    "    #return the confusion matrix \n",
    "    return  confusion_matrix(trues, guesses)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1: \n",
    "\n",
    "The unsuperivsed niave bayes (NB) has no concept of \"classes\" and therefore the classes can sometime end up \"swapped\" therefore reducing the overall accuracy for that unsupervised NB instance. Another factor which effects the the accuracy of the classifier is the proportion of nominal attributes compared to the number of instances. The reason why the unsupervised NB works at all is because we initially generated non-uniform data which is similar to the non-uniform data provided. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8044066962087641\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filepath = \"./datasets/mushroom-dos.csv\"\n",
    "\n",
    "#driver made to preprocess train and evaluate an unsupervised classifier\n",
    "def unsuper_driver():\n",
    "    unsuper_df, classes = unsupervised_preprocess()\n",
    "    probList, classes, priors = train_unsupervised(unsuper_df, classes)\n",
    "    testdf = pd.read_csv(filepath, header = None)\n",
    "   \n",
    "\n",
    "    cm = evaluate_unsupervised(testdf,probList,classes,priors)\n",
    "    totaltrue = 0\n",
    "    total = 0\n",
    "    #calculate accuracy for classifier \n",
    "    for row in cm.transpose():\n",
    "        totaltrue = totaltrue + max(row)\n",
    "        total = total + sum(row)\n",
    "    print(totaltrue/total)\n",
    "unsuper_driver()\n",
    "\n",
    "#print(str(unsuper_driver()*100)+'%' + \" Accuracy for unsupervised algorithm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "\n",
    "The performance of the supervised classifier tends to work better with more instances (rows) in the dataset. This however is not the case between \"breast-cancer\" and \"cars\" datasets, however this variation could be due to cars having a higher number of classes which causes its accuracy to be lower despite having more instances overall. \n",
    "\n",
    "furthermore another accuracy influence could be due to the higher number of nominal attributes allows the classifier to be more accurate. This could be explained by the classifiers ability to be more specific due to the higher number of attributes. For example the chance of a combination of 22 attributes occuring twice in the same dataset is low, therefore the classifier will most likely recognise the class of the instance with those attributes if it needs to predict it since there will be few other options that match.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.02314814814815% Accuracy for supervised algorithm\n"
     ]
    }
   ],
   "source": [
    "filepath = \"./datasets/car-dos.csv\"\n",
    "#driver made to preprocess train and evaluate a  supervised\n",
    "def simple_driver():\n",
    "\n",
    "    df = pd.read_csv(filepath, header = None)\n",
    "  \n",
    "    probs, classes, priors = train_supervised(df)\n",
    "    \n",
    "    return evaluate_supervised(df, probs,classes,priors)\n",
    "print(str(simple_driver()*100)+'%' + \" Accuracy for supervised algorithm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "\n",
    "The accuracy of testing using k-fold cross-validation method was quite close to the accuracy of testing on training data. As the number of K increases the accuracy converges with that of Training on test data. However the processing time also increases exponentially and therefore there is a tradeoff between accuracy rating and CPU time. This is most prominent when using the \"Hold one\" method where K is the datasets size less 1. Where the accuracies are equal however the training time is unfeasible for lager datasets. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.01747546713268% Accuracy for supervised algorithm using k-fold cross validation\n",
      "70.02314814814815% Accuracy for supervised algorithm without using cross validation\n",
      "quite close in accuracy\n"
     ]
    }
   ],
   "source": [
    "K  = 10 # k is the number of pieces to divide the data into must be atleast 2\n",
    "\n",
    "filepath = \"./datasets/car-dos.csv\"\n",
    "\n",
    "\n",
    "\n",
    "def driver():\n",
    "    fulldf = preprocess()\n",
    "    \n",
    "    return k_fold(fulldf)\n",
    "print(str(driver()*100)+'%' + \" Accuracy for supervised algorithm using k-fold cross validation\")\n",
    "    \n",
    "    \n",
    "def non_kfold_driver():\n",
    "\n",
    "    df = pd.read_csv(filepath, header = None)\n",
    "  \n",
    "    probs, classes,priors = train_supervised(df)\n",
    "    \n",
    "    return evaluate_supervised(df, probs,classes,priors)\n",
    "print(str(non_kfold_driver()*100)+'%' + \" Accuracy for supervised algorithm without using cross validation\")\n",
    "\n",
    "print(\"quite close in accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "\n",
    "By deterministically assigning a random class to each instance. The accuracy will converge to 1/classes as instances increase. In contrast, if you instead assign a non-uniform distribution to each instance, through iterations the data's distrubution will become closer to the distrubution of the original dataset and therefore has potential to be much higher accuracy than the deterministic approach that is 1/classes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.93265886816314% Accuracy for unsupervised deterministic algorithm without using cross validation\n",
      "27.256944444444443% Accuracy for unsupervised deterministic algorithm without using cross validation\n",
      "53.14685314685315% Accuracy for unsupervised deterministic algorithm without using cross validation\n",
      "^^ trend can be identified as 1/classes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filepath = \"./datasets/hypothyroid-dos.csv\"\n",
    "\n",
    "def deterministic_driver():\n",
    "    df, classes = deterministic_preprocess()\n",
    "  \n",
    "    probs, classes,priors = train_supervised(df)\n",
    "    \n",
    "    return evaluate_supervised(df, probs,classes,priors)\n",
    "\n",
    "print(str(deterministic_driver()*100)+ \"%\" + \" Accuracy for unsupervised deterministic algorithm without using cross validation\")\n",
    "\n",
    "filepath = \"./datasets/car-dos.csv\"\n",
    "\n",
    "print(str(deterministic_driver()*100)+ \"%\" + \" Accuracy for unsupervised deterministic algorithm without using cross validation\")\n",
    "\n",
    "filepath = \"./datasets/breast-cancer-dos.csv\"\n",
    "\n",
    "print(str(deterministic_driver()*100)+ \"%\" + \" Accuracy for unsupervised deterministic algorithm without using cross validation\")\n",
    "\n",
    "print(\"^^ trend can be identified as 1/classes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
