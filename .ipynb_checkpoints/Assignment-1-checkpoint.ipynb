{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bilal Shehata (834720) and Armaan Mcleoud (837674)\n",
    "--DREAM TEAM--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS CELL TO LOAD IN FUNCTIONS\n",
    "\n",
    "#imports used for program\n",
    "\n",
    "import itertools\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "from random import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "EPSILON = 0.00000000000001\n",
    "    \n",
    "#preprocessing for supervised classifier \n",
    "def preprocess():\n",
    "    \n",
    "    \n",
    "    #load in data \n",
    "    df = pd.read_csv(filepath, header = None)\n",
    "\n",
    "    clean(df) #impute missing values if they exist\n",
    "    df = df.sample(frac=1).reset_index(drop=True) #shuffles the dataframe\n",
    "  \n",
    "    \n",
    "    return df\n",
    "\n",
    "               \n",
    "           \n",
    "def train_supervised(df):\n",
    "    \n",
    "    #prior probabilties\n",
    "    highProb = df.groupby(len(df.columns)-1).size().div(len(df))\n",
    "     \n",
    "    #probabiltiy deictionary \n",
    "    probList = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    #this loop calculates the probabilties \n",
    "    for i in range(0,len(df.columns)-1):\n",
    "        cols =[len(df.columns)-1,i]\n",
    "       \n",
    "        trained = df.groupby(cols).size().div(len(df)).multiply(highProb, axis = 0,level=(len(df.columns)-1))\n",
    "        \n",
    "        probList[i] = trained\n",
    "    \n",
    "    #converts dataframe into dictionary \n",
    "    for i in range(0,len(probList)):\n",
    "        probList[i] = probList[i].to_dict()\n",
    "        #collects the classes into an list\n",
    "    classes = df[len(df.columns)-1].unique()\n",
    "    \n",
    "    priors = get_super_priors(df,classes)\n",
    "    \n",
    "    return probList, classes, priors\n",
    "\n",
    "def evaluate_supervised(testcsv, probs,classes,priors):\n",
    "\n",
    "    correct = 0\n",
    "    total = 0 \n",
    "    #iterate over each of the rows and pass it to the predict supervised method\n",
    "    for index, testrow in testcsv.iterrows() :\n",
    "        \n",
    "        #if the prediction is correct increase the counter \n",
    "        if predict_supervised(probs, testrow.tolist(),classes,priors) == testrow[len(testcsv.columns)-1]:\n",
    "            \n",
    "            correct +=1\n",
    "     \n",
    "        total +=1\n",
    "        \n",
    "     #return the accuracy of the classifier \n",
    "    return correct/total\n",
    "            \n",
    "#cleaning methiod removes '?' and places in it the most common value for that column\n",
    "def clean(dataframe):\n",
    "    #find all ? values and impute the most common value for that attribute \n",
    "    for index, testrow in dataframe.iterrows():\n",
    "      for i in range(0,len(testrow)):\n",
    "          if testrow[i] == '?':\n",
    "              testrow[i] = dataframe[i].value_counts().idxmax()\n",
    "              dataframe.loc[index,dataframe.columns[i] ] = testrow[i]\n",
    "  \n",
    "    return dataframe\n",
    "     \n",
    "\n",
    "    \n",
    "    #train the data\n",
    "  \n",
    "\n",
    "#function is used for preprocessing the data for unsupervised method \n",
    "def unsupervised_preprocess():\n",
    "      df = pd.read_csv(filepath, header = None)\n",
    "      \n",
    "      df = clean(df) #impute missing values if they exist\n",
    "      df = df.sample(frac=1).reset_index(drop=True) #shuffles the dataframe\n",
    "      classes = sorted(df[len(df.columns)-1].unique()) #returns the set of classes from dataframe\n",
    "   \n",
    "      df = df.iloc[:, :-1] #for unsupervised method the classes are cropped off the dataframe \n",
    "      #for each row and class assign a random value to the classes\n",
    "      for probable_class in classes:\n",
    "          df[probable_class] = pd.Series(np.random.rand(len(df)))\n",
    "      #ensure that the values are normalised\n",
    "      normalise_unsupervised(df,classes)\n",
    "      \n",
    "   \n",
    "      return df, classes\n",
    "#function is used to normalise the distributions of the pandas file in the dataframe     \n",
    "def normalise_unsupervised(df, classes):\n",
    "    #iterate through the rows\n",
    "    for index, row in df.iterrows():\n",
    "        #iterate through the attributes which are classes\n",
    "        total = sum(row[-len(classes):])\n",
    "        for i in range((len(row)-len(classes)),len(row)):\n",
    "            temp= float(row[i])/total\n",
    "            df.loc[index,df.columns[i] ] = temp\n",
    "      \n",
    "   \n",
    "    return\n",
    "\n",
    "#function to return the prior probablities in the supervised method\n",
    "def get_super_priors(df,classes):\n",
    "    priors = [0.0] * len(classes)\n",
    "    total = 0\n",
    "    classes = classes.tolist()\n",
    "    for index, row in df.iterrows():\n",
    "        priors[classes.index(row[len(row)-1])] += 1 \n",
    "        total += 1\n",
    "    for i in range(len(priors)):\n",
    "        priors[i] = priors[i]/total\n",
    "\n",
    "           \n",
    "      \n",
    "   \n",
    "    return priors\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# This function should build an unsupervised NB model \n",
    "def train_unsupervised(df, classes):\n",
    "   \n",
    "    #collect priors \n",
    "    priors= get_priors(df,classes)\n",
    "    \n",
    "    #this value indictates the number of iterations  \n",
    "    iterations = 4\n",
    "    for i in range(1,iterations): \n",
    "       #create the initial dictionary \n",
    "        probList = make_probability_dictionary(df,classes)\n",
    "        #use divide the values by fractional values \n",
    "        probList = factional_divide(df,classes,probList)\n",
    "        #reassign the distrubitions \n",
    "        df = assign_distro(df,classes,probList,priors)\n",
    "\n",
    "    \n",
    "    return probList, classes, priors\n",
    "\n",
    "\n",
    "#this function edits the probablity dictionary by fractionally diving the value s\n",
    "def factional_divide(df,classes,probList):\n",
    "    #get the fractional totals \n",
    "    fraccounts = get_fraccounts(df,classes)\n",
    "    #for every attribute\n",
    "    for i in probList.keys():\n",
    "        #for every class\n",
    "        for classtype in probList[i].keys():\n",
    "            #for every attribute value \n",
    "            for value in probList[i][classtype].keys():\n",
    "                    #divide the probability by the fractional count for that probabily \n",
    "                    probList[i][classtype][value] = probList[i][classtype][value] / fraccounts[classes.index(classtype)]\n",
    "\n",
    "    return probList\n",
    "\n",
    "#creates a list of fractional counts \n",
    "def get_fraccounts(df,classes):\n",
    "    fraccounts = []\n",
    "    for i in classes:   \n",
    "        #add the sum of each class column to list \n",
    "        fraccounts.append(sum(df[i]))\n",
    "    \n",
    "    return fraccounts\n",
    "#this function builds a probability dictionary \n",
    "def make_probability_dictionary(df,classes):\n",
    "    \n",
    "     probList = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda:EPSILON)))\n",
    "     \n",
    "     for prob_class in range(0,len(classes)):\n",
    "        for index, row in df.iterrows() :\n",
    "            for i in range(0,len(row)-len(classes)):\n",
    "                probList[i][classes[prob_class]][row[i]] +=row[len(row)-len(classes)+prob_class]\n",
    "     return probList\n",
    "\n",
    "#assigns the distrobution for unsupervised instances \n",
    "def assign_distro(df,classes,probList,classProbs):\n",
    "     for index, row in df.iterrows():\n",
    "       \n",
    "       #iterate through the instances assigning new values for each class \n",
    "       for probable_Class in range(len(classes)):\n",
    "           \n",
    "           product = 1 * classProbs[probable_Class]\n",
    "           \n",
    "           for attrib in range((len(row)-len(classes))):    \n",
    "              \n",
    "               product = product *probList[attrib][classes[probable_Class]][row[attrib]]\n",
    "               \n",
    "           df.loc[index,df.columns[(len(row)-len(classes) + probable_Class)]] = product\n",
    "    #normalise the values so they add to 1 \n",
    "     normalise_unsupervised(df,classes)\n",
    "     \n",
    "     return df\n",
    "\n",
    "#returns the values for the prior probablities \n",
    "def get_priors(df,classes):\n",
    "    classProbs = []\n",
    "    #create a list for the probablities for each of the classes \n",
    "    for i in classes:   \n",
    "        \n",
    "        classProbs.append(sum(df[i])/len(df))\n",
    "    \n",
    "    return classProbs\n",
    "#class makes predicitons for supervised method \n",
    "def predict_supervised(probList,testrow,classes,priors):\n",
    "    \n",
    "    classChance = [0.0] * len(classes)#keep record of all classchanes for this row\n",
    "    classlist = classes.tolist()\n",
    "    for possibleClass in classes : \n",
    "        #multiple the prior probablity \n",
    "        prob =1 *priors[classlist.index(possibleClass)]\n",
    "        for i in range(0,len(testrow)-1) : #for every element multiply in its conditional probablity \n",
    "  \n",
    "            if (possibleClass,testrow[i]) in probList[i]: \n",
    "                #multiply in condition probability \n",
    "                prob = prob * probList[i][(possibleClass,testrow[i])]\n",
    "            else : \n",
    "               \n",
    "                prob = prob * EPSILON #epsilon \n",
    "        \n",
    "        classChance[classlist.index(possibleClass)]= prob\n",
    "        \n",
    "        \n",
    "\n",
    "            #here we get the highest probability and match it to the class\n",
    "    \n",
    "    #the class with the highest probablity is what is predicted  \n",
    "    return classes[classChance.index(max(classChance))]\n",
    "\n",
    "# This function should predict the class distribution for a set of instances, based on a trained model\n",
    "def predict_unsupervised(probList,testrow,classes,priors):\n",
    "    \n",
    "    shuffle(classes)  \n",
    "    classChance = [0.0] * len(classes)#keep record of all classchanes for this row\n",
    "    classlist = classes\n",
    "    for possibleClass in classes : \n",
    "        #multiply in prior probability  \n",
    "        prob =1 *priors[classlist.index(possibleClass)]\n",
    "        for i in range(0,len(testrow)-len(classes)) : #for every element multiply in  its conditional probability \n",
    "  \n",
    "            if testrow[i] in probList[i][possibleClass]: \n",
    "                  #multiply in the condition probablity \n",
    "                prob = prob * probList[i][possibleClass][testrow[i]]\n",
    "            else: \n",
    "               \n",
    "               prob = prob * EPSILON #epsilon \n",
    "        classChance[classlist.index(possibleClass)]= prob\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    return classes[classChance.index(max(classChance))]\n",
    "  \n",
    "#uses confusion matrix to evaluate unsupervised classifier\n",
    "def evaluate_unsupervised(testcsv, probs, classes,priors):\n",
    "    \n",
    "    \n",
    "    correct = 0\n",
    "    total = 0 \n",
    "    guesses = list()\n",
    "    trues = list()\n",
    "    #iterate over each of the rows and pass it to the predict supervised method\n",
    "    for index, testrow in testcsv.iterrows() :\n",
    "        \n",
    "            \n",
    "        #correct predictions increase the correct count\n",
    "        if predict_unsupervised(probs, testrow.tolist(),classes,priors) == testrow[len(testcsv.columns)-1]:\n",
    "            \n",
    "            correct +=1\n",
    "        #add the guesses to a list\n",
    "        guesses.append(predict_unsupervised(probs, testrow.tolist(),classes,priors))\n",
    "        #add the true values to a list \n",
    "        trues.append(testrow[len(testcsv.columns)-1])\n",
    "        \n",
    "        total +=1\n",
    "    #this section generates a confusion matrix ----------------\n",
    "    cm = confusion_matrix(trues, guesses, labels=classes)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] #normalise the values \n",
    "    plt.imshow(cm,cmap= plt.cm.Greens) #use the green colour scheme \n",
    "  \n",
    "    plt.colorbar() #add the colour bar \n",
    "    tick_marks = np.arange(len(classes)) \n",
    "    plt.xticks(tick_marks,classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    width, height = cm.shape\n",
    "\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            plt.annotate('%.4f' % cm[x][y], xy=(y, x), #show 4 digits after decimal \n",
    "                        horizontalalignment='center',\n",
    "                        verticalalignment='center')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "        \n",
    "    #return the confusion matrix \n",
    "    return  confusion_matrix(trues, guesses)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1: \n",
    "\n",
    "The unsuperivsed niave bayes (NB) has no concept of \"classes\" and therefore the method to measure accuracy is modified, in this case we create a confusion matrix and calculate the accuracy by diving the max's of each column with the total number of instances. A factor which effects the the accuracy of the classifier was found to relate to the proportion of nominal attributes compared to the number of instances. It is suprirsing that unsupervised NB works at all given that we begin with randomly generated numbers however,  The reason why the unsupervised NB work is because we initially generated non-uniform distrubution  which is similar to the fact that real world data is non uniform. The iteration process aims to match the probability curve's such that the distribution of the answers is similar to the distribution of the training data which allows it to make predictions on  unknown instances. The probabilistic model changes from being a random set of generated numbers to a distribution curve which is alikend to the training data. This model would perform worse if the training data distrubution is not representative of the testing data's true class distributions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7973904480551453% Accuracy for unsupervised method\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filepath = \"./datasets/mushroom-dos.csv\"  #file path can be adjusted to any of the datasets provided\n",
    "\n",
    "#driver made to preprocess train and evaluate an unsupervised classifier\n",
    "def unsuper_driver():\n",
    "    unsuper_df, classes = unsupervised_preprocess()\n",
    "    probList, classes, priors = train_unsupervised(unsuper_df, classes)\n",
    "    testdf = pd.read_csv(filepath, header = None)\n",
    "   \n",
    "\n",
    "    cm = evaluate_unsupervised(testdf,probList,classes,priors)\n",
    "    totaltrue = 0\n",
    "    total = 0\n",
    "    #calculate accuracy for classifier \n",
    "    for row in cm.transpose():\n",
    "        totaltrue = totaltrue + max(row)\n",
    "        total = total + sum(row)\n",
    "    print(str(totaltrue/total) + \"% Accuracy for unsupervised method\")\n",
    "unsuper_driver()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "\n",
    "there is a clear correlation between The performance of the supervised classifier and data with more instances (rows) in the dataset, this can be due to the classifier having more information to build the model upon. This however is not the case between \"breast-cancer\" and \"cars\" datasets as cars has more instances yet the model performs better on \"breast-cancer\" which is much fewer, however this variation could be due to cars having a higher number of classes which causes its accuracy to be lower despite having more instances overall. This phenoma could be caused by the model requiring more information to build a satisfactory classifier when dealing with larger number of classes.\n",
    "\n",
    "furthermore another accuracy influence could be due to the higher number of nominal attributes allows the classifier to be more precise. This could be explained by the classifiers ability to be more specific due to the higher number of attributes. For example the chance of a combination of 22 attributes occuring twice in the same dataset is low, therefore the classifier will most likely recognise the class of the instance with those attributes if it needs to predict it since there will be few other intances in the past that match hence the classifier wont be as \"confused\" (i.e. 1 classes probability is likely to be much higher than the others).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.02314814814815% Accuracy for supervised algorithm\n"
     ]
    }
   ],
   "source": [
    "filepath = \"./datasets/car-dos.csv\" #file path can be adjusted to any of the datasets provided\n",
    "#driver made to preprocess train and evaluate a  supervised\n",
    "def simple_driver():\n",
    "\n",
    "    df = pd.read_csv(filepath, header = None)\n",
    "  \n",
    "    probs, classes, priors = train_supervised(df)\n",
    "    \n",
    "    return evaluate_supervised(df, probs,classes,priors)\n",
    "print(str(simple_driver()*100)+'%' + \" Accuracy for supervised algorithm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "\n",
    "The accuracy of testing using k-fold cross-validation method was quite close to the accuracy of testing on training data. As the number of K increases the accuracy converges with that of Training on test data. However the processing time also increases exponentially and therefore there is a tradeoff between accuracy rating and CPU time. This is most prominent when using the \"Hold one\" method where K is the datasets length less 1. The difference between the accuracies exponentially decreases as the values converge however cpu time increases exponentially therefore it is not feasible to use this method with high K values as it would not have an affect on the accuracy rating as much as it would on cpu time. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.02150826724022% Accuracy for supervised algorithm using k-fold cross validation\n",
      "70.02314814814815% Accuracy for supervised algorithm without using cross validation\n",
      "quite close in accuracy\n"
     ]
    }
   ],
   "source": [
    "K  = 10 # k is the number of pieces to divide the data into must be atleast 2\n",
    "\n",
    "filepath = \"./datasets/car-dos.csv\" #file path can be adjusted to any of the datasets provided\n",
    "\n",
    "def k_fold(fulldf):\n",
    "    #split array into 10 pieces\n",
    "    karrays = np.array_split(fulldf,K)\n",
    "  \n",
    "    counter = 0 \n",
    "    sum = 0 #record results\n",
    "    \n",
    "    for i in range(0,len(karrays)):\n",
    "        counter = 0\n",
    "        testdf = karrays[i] #set the test array as one of the chunks\n",
    "        \n",
    "       \n",
    "        for j in range(0,len(karrays)):\n",
    "            #ensure that were not adding the test chunk to the array \n",
    "            if i != j: \n",
    "                \n",
    "               if counter == 0:\n",
    "                   counter+=1\n",
    "                   traindf = karrays[j] #initialise the data to be trained\n",
    "                   continue \n",
    "               #concatinate all the chunks that arent the test chunk\n",
    "               traindf = pd.concat([traindf,karrays[j]],axis = 0) \n",
    "        \n",
    "        #train the classifer by building the probability dictionary \n",
    "        probs, classes, priors = train_supervised(traindf)\n",
    "        #evaluate the classifier \n",
    "    \n",
    "        sum+= evaluate_supervised(testdf, probs, classes,priors)\n",
    "        \n",
    "    return sum/K       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def driver():\n",
    "    fulldf = preprocess()\n",
    "    \n",
    "    return k_fold(fulldf)\n",
    "print(str(driver()*100)+'%' + \" Accuracy for supervised algorithm using k-fold cross validation\")\n",
    "    \n",
    "    \n",
    "def non_kfold_driver():\n",
    "\n",
    "    df = pd.read_csv(filepath, header = None)\n",
    "  \n",
    "    probs, classes,priors = train_supervised(df)\n",
    "    \n",
    "    return evaluate_supervised(df, probs,classes,priors)\n",
    "print(str(non_kfold_driver()*100)+'%' + \" Accuracy for supervised algorithm without using cross validation\")\n",
    "\n",
    "print(\"quite close in accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "\n",
    "By deterministically assigning a random class to each instance based on no attribute will cause the classifier to not build any relationship between attributes and class. therefore the  probability that any instance in the training data set will be a particular class will be approximately 1/classes. This method will perform poorly as it is not taking into consideration any information about the data and just arbitrarily assigning it a class. The test data will therefore follow the same pattern in receiving an arbitrary class when being classified which it will have a probability of 1/classes in being correct. We have determined that Unsupervised NB using distributions can perform better than this therefore determinisitically  assigning an attribute is not as good as assigning distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.944356623458745% Accuracy for unsupervised deterministic algorithm without using cross validation\n",
      "27.025462962962965% Accuracy for unsupervised deterministic algorithm without using cross validation\n",
      "52.7972027972028% Accuracy for unsupervised deterministic algorithm without using cross validation\n",
      "^^ trend can be identified as 1/classes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filepath = \"./datasets/hypothyroid-dos.csv\" #file path can be adjusted to any of the datasets provided\n",
    "#determinisatically decide classes for instances \n",
    "def deterministic_preprocess():\n",
    "    #load in the dataframe \n",
    "     df = pd.read_csv(filepath, header = None)\n",
    "    \n",
    "     clean(df) #impute missing values if they exist\n",
    "     df = df.sample(frac=1).reset_index(drop=True) #shuffles the dataframe\n",
    "     classes = df[len(df.columns)-1].unique()\n",
    "    #remove classes from the data \n",
    "     df = df.iloc[:, :-1]\n",
    "    \n",
    "    \n",
    "    #randomly select a class for the instance \n",
    "     df[len(df.columns)-1] = np.random.choice(classes, df.shape[0])\n",
    "     \n",
    "    \n",
    "   \n",
    "   \n",
    "     return df, classes\n",
    "    \n",
    "#function utilises the deterministic process to build distrubutions \n",
    "def deterministic_driver():\n",
    "    df, classes = deterministic_preprocess()\n",
    "  \n",
    "    probs, classes,priors = train_supervised(df)\n",
    "    \n",
    "    return evaluate_supervised(df, probs,classes,priors)\n",
    "\n",
    "\n",
    "#validating the results across datasets with varying classes\n",
    "print(str(deterministic_driver()*100)+ \"%\" + \" Accuracy for unsupervised deterministic algorithm without using cross validation\")\n",
    "\n",
    "filepath = \"./datasets/car-dos.csv\"\n",
    "\n",
    "print(str(deterministic_driver()*100)+ \"%\" + \" Accuracy for unsupervised deterministic algorithm without using cross validation\")\n",
    "\n",
    "filepath = \"./datasets/breast-cancer-dos.csv\"\n",
    "\n",
    "print(str(deterministic_driver()*100)+ \"%\" + \" Accuracy for unsupervised deterministic algorithm without using cross validation\")\n",
    "\n",
    "print(\"^^ trend can be identified as 1/classes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
