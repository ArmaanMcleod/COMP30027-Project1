{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import csv\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "from random import shuffles \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1: \n",
    "\n",
    "The unsuperivsed niave bayes (NB) has no concept of \"classes\" and therefore the classes can sometime end up \"swapped\" therefore reducing the overall accuracy for that unsupervised NB instance. Another factor which effects the the accuracy of the classifier is the proportion of nominal attributes compared to the number of instances. The reason why the unsupervised NB works at all is because we initially generated non-uniform data which is similar to the non-uniform data provided. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "\n",
    "The performance of the supervised classifier tends to work better with more instances (rows) in the dataset. This however is not the case between \"breast-cancer\" and \"cars\" datasets, however this variation could be due to cars having a higher number of classes which causes its accuracy to be lower despite having more instances overall. \n",
    "\n",
    "furthermore another accuracy influence could be due to the higher number of nominal attributes allows the classifier to be more accurate. This could be explained by the classifiers ability to be more specific due to the higher number of attributes. For example the chance of a combination of 22 attributes occuring twice in the same dataset is low, therefore the classifier will most likely recognise the class of the instance with those attributes if it needs to predict it since there will be few other options that match.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "\n",
    "The accuracy of testing using k-fold cross-validation method was quite close to the accuracy of testing on training data. As the number of K increases the accuracy converges with that of Training on test data. However the processing time also increases exponentially and therefore there is a tradeoff between accuracy rating and CPU time. This is most prominent when using the \"Hold one\" method where K is the datasets size less 1. Where the accuracies are equal however the training time is unfeasible for lager datasets. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "\n",
    "By deterministically assigning a random class to each instance. The accuracy will converge to 1/classes as instances increase. In contrast, if you instead assign a non-uniform distribution to each instance, through iterations the data's distrubution will become closer to the distrubution of the original dataset and therefore has potential to be much higher accuracy than the deterministic approach that is 1/classes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
