{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from csv import reader\n",
    "from pprint import pprint\n",
    "from itertools import chain\n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# This function should open a data file in csv\n",
    "# transform it into a usable format\n",
    "def preprocess(filename):\n",
    "\n",
    "    # all lines in file go here\n",
    "    lines = []\n",
    "\n",
    "    # open file\n",
    "    with open(filename) as infile:\n",
    "\n",
    "        # convert to reader object\n",
    "        csv_reader = reader(infile)\n",
    "\n",
    "        # loop over each line and add it to resultOnant list\n",
    "        for line in csv_reader:\n",
    "            lines.append(line)\n",
    "\n",
    "    return lines\n",
    "\n",
    "# This function should build a supervised NB model\n",
    "def train_supervised(training_data):\n",
    "\n",
    "    # priors count dictionary\n",
    "    priors = defaultdict(int)\n",
    "\n",
    "    # count prior probabilities\n",
    "    for line in training_data:\n",
    "        col = line[-1]\n",
    "        priors[col] += 1/len(training_data)\n",
    "\n",
    "    # create posteriers data structure with triple nested defaultdicts\n",
    "    # perhaps a more simplified data structure could be used here\n",
    "    # initialising int here initially for storing frequencies\n",
    "    # initialise frequency to 1 for smoothing\n",
    "    posteriers = defaultdict(lambda : defaultdict(lambda : defaultdict(int)))\n",
    "\n",
    "    # Loop over each line in preprocessed training data\n",
    "    for line in training_data:\n",
    "\n",
    "        # obtain class and attributes\n",
    "        attributes, class_name = line[:-1], line[-1]\n",
    "\n",
    "        # count each item\n",
    "        # using indices as attribute headers since not given in dataset\n",
    "        # indice starts at one\n",
    "        # perhaps datasets could be modified to include header names\n",
    "        for attribute, freq in enumerate(attributes):\n",
    "            posteriers[class_name][attribute][freq] += 1\n",
    "\n",
    "    # transform posteriers counts into probabilities\n",
    "    for class_name in posteriers:\n",
    "        for attribute in posteriers[class_name]:\n",
    "\n",
    "            # sum counts over each dict\n",
    "            # This will be the same accross each class\n",
    "            sums = sum(posteriers[class_name][attribute].values())\n",
    "\n",
    "            # update to probabilites by dividing freq/sums\n",
    "            for freq in posteriers[class_name][attribute]:\n",
    "                posteriers[class_name][attribute][freq] /= sums\n",
    "\n",
    "    # return a tuple of the two above data structures\n",
    "    return priors, posteriers\n",
    "\n",
    "# This function should predict the class for a set of instances\n",
    "# based on a trained model\n",
    "def predict_supervised(priors, posteriers, instance):\n",
    "\n",
    "    # epsilon value for smoothing\n",
    "    EPSILON = 0.000000000001\n",
    "\n",
    "    # dictionary holding maximal for each class\n",
    "    class_max = {}\n",
    "\n",
    "    for class_name in posteriers:\n",
    "        product = 1\n",
    "\n",
    "        # go over attributes of the trained data and instances at once\n",
    "        # get the hashed probability\n",
    "        for attribute, value in zip(posteriers[class_name], instance):\n",
    "            prob = posteriers[class_name][attribute][value]\n",
    "\n",
    "            # if probability is non-zero, accumulate it\n",
    "            # otherwise, accumulate the smoothing factor\n",
    "            if prob:\n",
    "                product *= prob\n",
    "            else:\n",
    "                product *= EPSILON\n",
    "\n",
    "        class_max[class_name] = product * priors[class_name]\n",
    "\n",
    "    # return a tuple of the class and maximal probability\n",
    "    return max(class_max.items(), key=itemgetter(1))\n",
    "\n",
    "# This function should evaluate a set of predictions, in a supervised context\n",
    "def evaluate_supervised(priors, posteriers, training_data):\n",
    "\n",
    "    # keep a counter of correct instances found\n",
    "    correct = 0\n",
    "\n",
    "    # go over each instance in data set\n",
    "    # get predicted class for each instance\n",
    "    for instance in training_data:\n",
    "        predict_class, _ = predict_supervised(priors, posteriers, instance)\n",
    "\n",
    "        # if class is identical to the instances last column\n",
    "        # increment the count\n",
    "        if predict_class == instance[-1]:\n",
    "            correct += 1\n",
    "\n",
    "    return correct/len(training_data)\n",
    "\n",
    "# This function uses the cross validation strategy\n",
    "# Which runs on a supervised NB model\n",
    "def cross_validation(dataset, k):\n",
    "\n",
    "    # divide dataset into k length partitions\n",
    "    partitions = [part.tolist() for part in np.array_split(dataset, k)]\n",
    "\n",
    "    # helper function for flattening a list\n",
    "    flatten = lambda lst : list(chain.from_iterable(lst))\n",
    "\n",
    "    # accuracy counter\n",
    "    sums = 0\n",
    "\n",
    "    for i, test_data in enumerate(partitions):\n",
    "\n",
    "        # get every other partition except current test data\n",
    "        training_data = flatten(partitions[:i]) + flatten(partitions[i+1:])\n",
    "\n",
    "        # get the trained supervised model\n",
    "        priors, posteriers = train_supervised(training_data)\n",
    "\n",
    "        # accumulate accuracy of test data\n",
    "        sums += evaluate_supervised(priors, posteriers, test_data)\n",
    "\n",
    "    return sums/k\n",
    "\n",
    "# This function creates a postierer count dictionary\n",
    "def construct_posteriers_unsupervised(priors, distributions, training_data):\n",
    "    # new posteriers\n",
    "    posteriers = defaultdict(lambda : defaultdict(lambda : defaultdict(float)))\n",
    "\n",
    "    # convert fractional counts to probabilities\n",
    "    for instance, dist_dict in zip(training_data, distributions):\n",
    "        for class_name, dist in dist_dict.items():\n",
    "            for attribute, value in enumerate(instance):\n",
    "                posteriers[class_name][attribute][value] += (dist / \n",
    "                                                            priors[class_name])\n",
    "\n",
    "    return posteriers\n",
    "\n",
    "# This function cleans training data\n",
    "def clean(training_data):\n",
    "    cleansed_data = []\n",
    "\n",
    "    # transpose training data for imputation\n",
    "    # make columns rows instead for easier access and manipulation\n",
    "    for column in zip(*training_data):\n",
    "\n",
    "        # count elements in each column\n",
    "        # remove '?' character from counts\n",
    "        # get most common item in column\n",
    "        counts = Counter(column)\n",
    "        counts.pop('?', None)\n",
    "        common = counts.most_common(1)[0][0]\n",
    "\n",
    "        # imputate invalid data with max occuring data in column\n",
    "        new_column = [common if val == '?' else val for val in column]\n",
    "        cleansed_data.append(new_column)\n",
    "\n",
    "    # transpose modified data back into rows\n",
    "    return list(map(list, zip(*cleansed_data)))\n",
    "\n",
    "# This function deterministically assigns classes for the supervised NB model\n",
    "def deterministic_supervised(training_data, classes):\n",
    "\n",
    "    # return random classes to each instance\n",
    "    return [inst[:-1] + [random.choice(classes)] for inst in training_data]\n",
    "\n",
    "# This function should build an unsupervised NB model\n",
    "def train_unsupervised(training_data):\n",
    "\n",
    "    # get sorted list of classes in data set\n",
    "    # allowed columns of classes to always be same order\n",
    "    classes = sorted(set(map(itemgetter(-1), training_data)))\n",
    "\n",
    "    # strip classes from training data\n",
    "    # also clean training data\n",
    "    classless_training = [instance[:-1] for instance in training_data]\n",
    "\n",
    "    # random_distributions\n",
    "    distributions = []\n",
    "\n",
    "    # priors of distribution counts\n",
    "    priors = defaultdict(float)\n",
    "\n",
    "    # calculate priors of random distribtions\n",
    "    # and transform training data into randomized labelled data\n",
    "    for instance in classless_training:\n",
    "        rand_distribution = np.random.dirichlet(np.ones(len(classes)), size=1)\n",
    "        \n",
    "        # add and create tupled pairs of (class, distribution)\n",
    "        # add as a ordered dictionary to keep ordering of classes\n",
    "        class_distribution = list(zip(classes, *rand_distribution))\n",
    "        distributions.append(OrderedDict(class_distribution))\n",
    "\n",
    "        # fraction prior counts\n",
    "        for class_name, dist in class_distribution:\n",
    "            priors[class_name] += dist\n",
    "\n",
    "    # transform into posterier probability dictionary\n",
    "    posteriers = construct_posteriers_unsupervised(priors, \n",
    "                                                   distributions, \n",
    "                                                   classless_training)\n",
    "\n",
    "    # return a tuple of all the needed data structures\n",
    "    return priors, posteriers, distributions, classless_training, classes\n",
    "\n",
    "# This function builds new predictions from an NB unsupervised model\n",
    "def predict_unsupervised(priors, posteriers, distributions, training_data):\n",
    "\n",
    "    # convert fractions to probabilities\n",
    "    probs = {cs: cnt / len(training_data) for cs, cnt in priors.items()}\n",
    "\n",
    "    # go over each instance and distribution sumultaneously\n",
    "    # collect new distributions\n",
    "    for instance, distribution in zip(training_data, distributions):\n",
    "        new_dists = []\n",
    "\n",
    "        # go over each class in distribution\n",
    "        for class_name in distribution:\n",
    "            product = 1\n",
    "\n",
    "            # calculate new distributions\n",
    "            for attribute, value in enumerate(instance):\n",
    "                product *= posteriers[class_name][attribute][value]\n",
    "\n",
    "            # multiply by probability and add it\n",
    "            product *= probs[class_name]\n",
    "            new_dists.append(product)\n",
    "\n",
    "        # normalise distribution into probabilities\n",
    "        for class_name, new_dist in zip(distribution, new_dists):\n",
    "            distribution[class_name] = new_dist / sum(new_dists)\n",
    "\n",
    "    # recreate posteriers with new distributions\n",
    "    new_posteriers = construct_posteriers_unsupervised(priors, \n",
    "                                                       distributions, \n",
    "                                                       training_data)\n",
    "\n",
    "    return new_posteriers, distributions\n",
    "\n",
    "# This function should create a confusion matrix\n",
    "def output_confusion_matrix(matches, guesses, classes):\n",
    "\n",
    "    # create confusion matrix\n",
    "    cm = confusion_matrix(matches, guesses, labels=classes)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cm,cmap= plt.cm.Greens)\n",
    "  \n",
    "    # set axis\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks,classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    # get dimensions of matrix\n",
    "    width, height = cm.shape\n",
    "\n",
    "    # annotate matrix\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            plt.annotate('%.4f' % cm[x][y], xy=(y, x), \n",
    "                        horizontalalignment='center',\n",
    "                        verticalalignment='center')\n",
    "\n",
    "    # create labels and show matrix\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    \n",
    "# This function should evaluate a set of predictions, in an unsupervised manner\n",
    "def evaluate_unsupervised(distributions, training_data, classes):\n",
    "\n",
    "    # keep a counter of correct instances found\n",
    "    correct = 0\n",
    "\n",
    "    # collect guesses and matched classes here\n",
    "    guesses = []\n",
    "    matches = []\n",
    "\n",
    "    # go over each instance ad parallel distribution in training data\n",
    "    # get predicted class for each instance\n",
    "    for instance, distribution in zip(training_data, distributions):\n",
    "        predict_class, _ = max(distribution.items(), key=itemgetter(1))\n",
    "        class_col = instance[-1]\n",
    "\n",
    "        # if class is identical to the instances last column\n",
    "        # increment the count\n",
    "        if predict_class == class_col:\n",
    "            correct += 1\n",
    "\n",
    "        # add predicted class and actual classes\n",
    "        # used for confusion matrix\n",
    "        guesses.append(predict_class)\n",
    "        matches.append(class_col)\n",
    "\n",
    "    # output a confusion matrix\n",
    "    output_confusion_matrix(matches, guesses, classes)\n",
    "\n",
    "    return correct/len(training_data)\n",
    "\n",
    "# This function gets all the csv files in the current directory\n",
    "def get_datasets(extension='.csv'):\n",
    "\n",
    "    files = []\n",
    "\n",
    "    # go through all items in current directorys\n",
    "    for file in os.listdir('.'):\n",
    "\n",
    "        # add if item is a file and ends with extension\n",
    "        if os.path.isfile(file) and file.endswith(extension):\n",
    "            files.append(file)\n",
    "\n",
    "    # return items in sorted order\n",
    "    return sorted(files)\n",
    "\n",
    "# This function is the main driver of program\n",
    "def main():\n",
    "    datasets = get_datasets()\n",
    "\n",
    "    for file in datasets:\n",
    "        data = preprocess(file)\n",
    "\n",
    "        #imputate invalid data\n",
    "        data = clean(data)\n",
    "\n",
    "        # SUPERVISED\n",
    "        priors, posteriers = train_supervised(data)\n",
    "\n",
    "        evaluate = evaluate_supervised(priors, posteriers, data)\n",
    "\n",
    "        print(file)\n",
    "        print('supervised NB accuracy: %f' % (evaluate))\n",
    "\n",
    "        # cross validation for supervised NB\n",
    "        K = 10\n",
    "        print('cross validation supervised: %f' % (cross_validation(data, K)))\n",
    "\n",
    "        # UNSUPERVISED\n",
    "        priors, posteriers, distributions, training_data, classes = train_unsupervised(data)\n",
    "\n",
    "        ITERATIONS = 2\n",
    "        count = 0\n",
    "        while count < ITERATIONS:\n",
    "            posterier_temp, distribution_temp = predict_unsupervised(priors,\n",
    "                                                                     posteriers, \n",
    "                                                                     distributions, \n",
    "                                                                     training_data)\n",
    "\n",
    "            posteriers, distributions = posterier_temp, distribution_temp\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        evaluate = evaluate_unsupervised(distributions, data, classes)\n",
    "\n",
    "        print('unsupervised NB accurary: %f' % (evaluate))\n",
    "\n",
    "        # UNSUPERVISED deterministic\n",
    "        data = deterministic_supervised(data, classes)\n",
    "        priors, posteriers = train_supervised(data)\n",
    "        evaluate = evaluate_supervised(priors, posteriers, data)\n",
    "        print('deterministic supervised accuracy: %f\\n' % (evaluate))\n",
    "        print('-----------------------------------------------------------------------')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "\n",
    "The unsuperivsed niave bayes (NB) has no concept of \"classes\" and therefore the method to measure accuracy is modified, in this case we create a confusion matrix and calculate the accuracy by diving the max's of each column with the total number of instances. A factor which effects the the accuracy of the classifier was found to relate to the proportion of nominal attributes compared to the number of instances. It is suprirsing that unsupervised NB works at all given that we begin with randomly generated numbers however, The reason why the unsupervised NB work is because we initially generated non-uniform distrubution which is similar to the fact that real world data is non uniform.\n",
    "\n",
    "the iteration process aims to match the probability curve's such that the distribution of the answers is similar to the distribution of the training data which allows it to make predictions on unknown instances. The probabilistic model changes from being a random set of generated numbers to a distribution curve which is alikend to the training data. This model would perform worse if the training data distrubution is not representative of the testing data's true class distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "\n",
    "The performance of the supervised classifier tends to work better with more instances (rows) in the dataset. This however is not the case between \"breast-cancer\" and \"cars\" datasets as cars has more instances, however this variation could be due to cars having a higher number of classes which causes its accuracy to be lower despite having more instances overall. This phenomena would suggest that the number of classes influences the perforamne of the classifier.\n",
    "\n",
    "furthermore another accuracy influence could be due to the higher number of nominal attributes allows the classifier to be more accurate. This could be explained by the classifiers ability to be more specific due to the higher number of attributes. For example the chance of a combination of 22 attributes occuring twice in the same dataset is low, therefore the classifier will most likely recognise the class of the instance with those attributes if it needs to predict it since there will be few other intances in the past that match hence the classifier wont be as \"confused\" (i.e. 1 classes probability is likely to be much higher than the others)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "\n",
    "The accuracy of testing using k-fold cross-validation method was quite close to the accuracy of testing on training data. As the number of K increases the accuracy converges with that of Training on test data. However the processing time also increases exponentially and therefore there is a tradeoff between accuracy rating and CPU time. This is most prominent when using the \"Hold one\" method where K is the datasets length less 1. The difference between the accuracies becomes exponentially small as the values converge however cpu time increases exponentially therefore it is not feasible to use this method with high K values as it would not have an affect on the accuracy rating as much as it would on cpu time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Question 4:\n",
    "\n",
    "By deterministically assigning a random class to each instance based on no attribute will cause the classifier to not build any relationship between attributes and class. therefore the probability that any instance in the training data set will be a particular class will be approximately 1/classes. This method will perform poorly as it is not taking into consideration any information about the data and just arbitrarily assigning it a class. The test data will therefore follow the same pattern in receiving an arbitrary class when being classified which it will have a probability of 1/classes in being correct."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
